{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  snippet_id                                    snippet_content\n",
      "0           0         968  ['past', 'week', 'republican', 'members_congre...\n",
      "1           1         969  ['european_union', 'long_criticized', 'east_eu...\n",
      "   claim_id  Unnamed: 0                                      claim_content\n",
      "0         9           0     ['taxes', 'spent', 'so_called', \"'war_terror\"]\n",
      "1        18         134  ['says', 'north_korea', 'agreed_denuclearizati...\n"
     ]
    }
   ],
   "source": [
    "df_bow = pd.read_csv('data/02_bow_snippets_claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We eval strings from snippets and claims as list of terms\n",
    "\n",
    "df_bow['snippet_content'] = df_bow['snippet_content'].apply(lambda st: ast.literal_eval(st))\n",
    "df_bow['claim_content'] = df_bow['claim_content'].apply(lambda st: ast.literal_eval(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>snippet_id</th>\n",
       "      <th>groundtruth_label</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>snippet_date</th>\n",
       "      <th>claim_content</th>\n",
       "      <th>snippet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>887759</td>\n",
       "      <td>True</td>\n",
       "      <td>4229</td>\n",
       "      <td>16577</td>\n",
       "      <td>We spend more money on lobbying than we do on ...</td>\n",
       "      <td>22/05/2015 · Will London's mayor put the brake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>887760</td>\n",
       "      <td>False</td>\n",
       "      <td>4229</td>\n",
       "      <td>16577</td>\n",
       "      <td>We spend more money on lobbying than we do on ...</td>\n",
       "      <td>22/05/2015 · ... Facebook for First Draft upda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  snippet_id  groundtruth_label  claim_id  snippet_date  \\\n",
       "0           0      887759               True      4229         16577   \n",
       "1           1      887760              False      4229         16577   \n",
       "\n",
       "                                       claim_content  \\\n",
       "0  We spend more money on lobbying than we do on ...   \n",
       "1  We spend more money on lobbying than we do on ...   \n",
       "\n",
       "                                     snippet_content  \n",
       "0  22/05/2015 · Will London's mayor put the brake...  \n",
       "1  22/05/2015 · ... Facebook for First Draft upda...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We import the ground truth data for relevance discovery\n",
    "\n",
    "df_groundtruth = pd.read_csv('data/01_relevance_discovery_groundtruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We merge groundtruth data with the bow representation of snippets and claims\n",
    "\n",
    "df_bow = df_bow[['snippet_id','snippet_content','claim_content']]\n",
    "df_bow.columns = ['snippet_id','snippet_bow','claim_bow']\n",
    "\n",
    "df_groundtruth = df_groundtruth[['claim_id','snippet_id','groundtruth_label']]\n",
    "df_merge = pd.merge(df_groundtruth, df_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each dictionary\n",
    "##     We transform the bow representation of snippets and claims into bow indices\n",
    "##     We trandform the bow representation of snippets and claims into tf-idf indices\n",
    "##     For each topic model\n",
    "##         We transform the bow or tf-idf indices into topic model indices \n",
    "\n",
    "for nb_word in sorted(list(np.arange(10000,40000,5000)),reverse=True):\n",
    "    dictionary = corpora.Dictionary.load('dictionaries/02_'+str(nb_word/1000)+'.dict')\n",
    "    \n",
    "    df_merge['snippet_bow_'+str(nb_word/1000)] = df_merge['snippet_bow'].apply(lambda x: dictionary.doc2bow(x))\n",
    "    tfidf = models.TfidfModel(list(df_merge['snippet_bow_'+str(nb_word/1000)].as_matrix()))\n",
    "    \n",
    "    df_merge['claim_bow_'+str(nb_word/1000)] = df_merge['claim_bow'].apply(lambda x: dictionary.doc2bow(x))\n",
    "    df_merge['claim_tfidf_'+str(nb_word/1000)] = df_merge['claim_bow_'+str(nb_word/1000)].apply(lambda x: tfidf[x])\n",
    "    \n",
    "    df_merge['snippet_tfidf_'+str(nb_word/1000)] = df_merge['snippet_bow_'+str(nb_word/1000)].apply(lambda x: tfidf[x])\n",
    "    \n",
    "    for nb_topic in np.arange(100,350,50):\n",
    "        \n",
    "        lsi = models.LsiModel.load('topic_models/'+str(nb_word/1000)+'_'+str(nb_topic)+'.lsi')\n",
    "        rp = models.RpModel.load('topic_models/'+str(nb_word/1000)+'_'+str(nb_topic)+'.rp')\n",
    "        lda = models.RpModel.load('topic_models/'+str(nb_word/1000)+'_'+str(nb_topic)+'.lda')\n",
    "        \n",
    "        df_merge['claim_lsi_'+str(nb_word/1000)+'_'+str(nb_topic)] = \\\n",
    "        df_merge['claim_tfidf_'+str(nb_word/1000)].apply(lambda x: lsi[x])\n",
    "        df_merge['claim_rp_'+str(nb_word/1000)+'_'+str(nb_topic)] = \\\n",
    "        df_merge['claim_bow_'+str(nb_word/1000)].apply(lambda x: rp[x])\n",
    "        df_merge['claim_lda_'+str(nb_word/1000)+'_'+str(nb_topic)] = \\\n",
    "        df_merge['claim_bow_'+str(nb_word/1000)].apply(lambda x: lda[x])\n",
    "        \n",
    "        df_merge['snippet_lsi_'+str(nb_word/1000)+'_'+str(nb_topic)] = \\\n",
    "        df_merge['snippet_tfidf_'+str(nb_word/1000)].apply(lambda x: lsi[x])\n",
    "        df_merge['snippet_rp_'+str(nb_word/1000)+'_'+str(nb_topic)] = \\\n",
    "        df_merge['snippet_bow_'+str(nb_word/1000)].apply(lambda x: rp[x])\n",
    "        df_merge['snippet_lda_'+str(nb_word/1000)+'_'+str(nb_topic)] = \\\n",
    "        df_merge['snippet_bow_'+str(nb_word/1000)].apply(lambda x: lda[x])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14203, 13683, 11160, 5779, 4338, 4229, 1085, 292, 187, 121]\n"
     ]
    }
   ],
   "source": [
    "## We retrieve a list of claim ids from the grountruth data\n",
    "\n",
    "claim_ids = sorted(list(df_merge['claim_id'].unique()),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each pair of claim id and snippet id from the grountruth data\n",
    "##    for each model (bow, tf-idf or topic model)\n",
    "##         We compute the cosinus similarity measure\n",
    "\n",
    "dic_similarities = {}\n",
    "\n",
    "dic_similarities['snippet_id'] = list(df_merge[['claim_id','snippet_id']]\\\n",
    ".sort_values(by=['claim_id','snippet_id'],ascending=False)['snippet_id'].as_matrix())\n",
    "\n",
    "for nb_word in sorted(list(np.arange(10000,40000,5000)),reverse=True):\n",
    "    \n",
    "    for nb_topic in np.arange(100,350,50):\n",
    "        lsi = models.LsiModel.load('topic_models/'+str(nb_word/1000)+'_'+str(nb_topic)+'.lsi')\n",
    "        rp = models.RpModel.load('topic_models/'+str(nb_word/1000)+'_'+str(nb_topic)+'.rp')\n",
    "        lda = models.RpModel.load('topic_models/'+str(nb_word/1000)+'_'+str(nb_topic)+'.lda')\n",
    "    \n",
    "        dic_similarities['lsi'+str(nb_word/1000)+'_'+str(nb_topic)] = []\n",
    "        dic_similarities['rp'+str(nb_word/1000)+'_'+str(nb_topic)] = []\n",
    "        dic_similarities['lda'+str(nb_word/1000)+'_'+str(nb_topic)] = []\n",
    "        print(\"Beginning nb topic = \"+str(nb_topic))\n",
    "        for claim_id in claim_ids:\n",
    "            df_ = df_merge[df_merge['claim_id']==claim_id][['snippet_id','snippet_bow_'+str(nb_word/1000),\n",
    "                                                                        'snippet_tfidf_'+str(nb_word/1000),\n",
    "                                                                    'claim_bow_'+str(nb_word/1000),\n",
    "                                                                        'claim_tfidf_'+str(nb_word/1000)]]\\\n",
    "            .sort_values(by=['snippet_id'],ascending=False)\n",
    "            claim_bow = list(df_['claim_bow_'+str(nb_word/1000)].as_matrix())[0]\n",
    "            claim_tfidf = list(df_['claim_tfidf_'+str(nb_word/1000)].as_matrix())[0]\n",
    "        \n",
    "            snippet_bow = list(df_['snippet_bow_'+str(nb_word/1000)].as_matrix())\n",
    "            snippet_tfidf = list(df_['snippet_tfidf_'+str(nb_word/1000)].as_matrix())\n",
    "        \n",
    "            lsi_index = similarities.MatrixSimilarity(lsi[snippet_tfidf])\n",
    "            rp_index = similarities.MatrixSimilarity(rp[snippet_bow])\n",
    "            lda_index = similarities.MatrixSimilarity(lda[snippet_bow])\n",
    "        \n",
    "            sims_lsi = lsi_index[lsi[claim_tfidf]]\n",
    "            sims_rp = rp_index[rp[claim_bow]]\n",
    "            sims_lda = lda_index[lda[claim_bow]]\n",
    "        \n",
    "            for i in range(0,len(snippet_bow)):\n",
    "                dic_similarities['lsi'+str(nb_word/1000)+'_'+str(nb_topic)].append(sims_lsi[i])\n",
    "                dic_similarities['rp'+str(nb_word/1000)+'_'+str(nb_topic)].append(sims_rp[i])\n",
    "                dic_similarities['lda'+str(nb_word/1000)+'_'+str(nb_topic)].append(sims_lda[i])\n",
    "    \n",
    "    dic_similarities['bow'+str(nb_word/1000)] = []\n",
    "    dic_similarities['tfidf'+str(nb_word/1000)] = []\n",
    "    for claim_id in claim_ids:\n",
    "        df_ = df_merge[df_merge['claim_id']==claim_id][['snippet_id','snippet_bow_'+str(nb_word/1000),\n",
    "                                                                        'snippet_tfidf_'+str(nb_word/1000),\n",
    "                                                                    'claim_bow_'+str(nb_word/1000),\n",
    "                                                                        'claim_tfidf_'+str(nb_word/1000)]]\\\n",
    "        .sort_values(by=['snippet_id'],ascending=False)\n",
    "        claim_bow = list(df_['claim_bow_'+str(nb_word/1000)].as_matrix())[0]\n",
    "        claim_tfidf = list(df_['claim_tfidf_'+str(nb_word/1000)].as_matrix())[0]\n",
    "        snippet_bow = list(df_['snippet_bow_'+str(nb_word/1000)].as_matrix())\n",
    "        snippet_tfidf = list(df_['snippet_tfidf_'+str(nb_word/1000)].as_matrix())\n",
    "        index_bow = similarities.MatrixSimilarity(snippet_bow)\n",
    "        index_tfidf = similarities.MatrixSimilarity(snippet_tfidf)\n",
    "        sims_bow = index_bow[claim_bow]\n",
    "        sims_tfidf = index_tfidf[claim_tfidf]\n",
    "        for i in range(0,len(snippet_bow)):\n",
    "            dic_similarities['bow'+str(nb_word/1000)].append(sims_bow[i])\n",
    "            dic_similarities['tfidf'+str(nb_word/1000)].append(sims_tfidf[i])\n",
    "            \n",
    "df_similarities = pd.DataFrame(dic_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We export the similarity measures\n",
    "\n",
    "df_results_final = pd.merge(df_merge[['snippet_id','groundtruth_label']], df_similarities)\n",
    "df_results_final.to_csv('datasets/04_similarity_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
