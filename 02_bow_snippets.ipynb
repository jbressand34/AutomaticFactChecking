{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "#https://www.datascience.com/resources/notebooks/word-embeddings-in-python\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>snippet_content</th>\n",
       "      <th>snippet_date</th>\n",
       "      <th>snippet_id</th>\n",
       "      <th>snippet_pagenum</th>\n",
       "      <th>snippet_title</th>\n",
       "      <th>snippet_url</th>\n",
       "      <th>claim_content</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>claim_tag</th>\n",
       "      <th>claim_url</th>\n",
       "      <th>date_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>News on Japan, Business News, Opinion, Sports,...</td>\n",
       "      <td>17636</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Article expired | The Japan Times</td>\n",
       "      <td>https://www.japantimes.co.jp/article-expired/</td>\n",
       "      <td>Black and Latino people in NYC are arrested at...</td>\n",
       "      <td>17646</td>\n",
       "      <td>True</td>\n",
       "      <td>‚Äî PolitiFact New York</td>\n",
       "      <td>/new-york/statements/2018/apr/25/kirsten-gilli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Get the latest breaking news across the U.S. o...</td>\n",
       "      <td>17636</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>U.S. News | Latest National News, Videos ‚Ä¶</td>\n",
       "      <td>https://abcnews.go.com/US/</td>\n",
       "      <td>Black and Latino people in NYC are arrested at...</td>\n",
       "      <td>17646</td>\n",
       "      <td>True</td>\n",
       "      <td>‚Äî PolitiFact New York</td>\n",
       "      <td>/new-york/statements/2018/apr/25/kirsten-gilli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                                    snippet_content  snippet_date  \\\n",
       "0         3  News on Japan, Business News, Opinion, Sports,...         17636   \n",
       "1         3  Get the latest breaking news across the U.S. o...         17636   \n",
       "\n",
       "   snippet_id  snippet_pagenum                               snippet_title  \\\n",
       "0           0                0           Article expired | The Japan Times   \n",
       "1           1                0  U.S. News | Latest National News, Videos ‚Ä¶   \n",
       "\n",
       "                                     snippet_url  \\\n",
       "0  https://www.japantimes.co.jp/article-expired/   \n",
       "1                     https://abcnews.go.com/US/   \n",
       "\n",
       "                                       claim_content  claim_date  claim_label  \\\n",
       "0  Black and Latino people in NYC are arrested at...       17646         True   \n",
       "1  Black and Latino people in NYC are arrested at...       17646         True   \n",
       "\n",
       "                claim_tag                                          claim_url  \\\n",
       "0  ‚Äî PolitiFact New York   /new-york/statements/2018/apr/25/kirsten-gilli...   \n",
       "1  ‚Äî PolitiFact New York   /new-york/statements/2018/apr/25/kirsten-gilli...   \n",
       "\n",
       "   date_number  \n",
       "0            0  \n",
       "1            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_snippets = pd.read_csv('data/00_dataset.csv')\n",
    "df_snippets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We preprocess quotes and double quotes in the content of our snippets and claims\n",
    "\n",
    "s_quotes = ['`','¬¥',' π',' ª',' º',' Ω','Àä','Àã','À¥','Õ¥','ŒÑ','’ô','’ö','’õ','’ú','’ù','‚Äò','‚Äô','‚Äõ','‚Ä≤','‚Äµ', 'ﬂ¥','◊ô' ,'◊≥', 'ﬂµ']\n",
    "d_quotes = [' ∫','Àù','ÀÆ','Àµ','À∂','◊≤','◊¥', '‚Äú', '‚Äù', '‚Äü', '‚Ä≥', '‚Ä¥', '‚Ä∂', '‚Ä∑','``']\n",
    "\n",
    "def replace_quotes(st):\n",
    "    global s_quotes, d_quotes\n",
    "    for c in s_quotes:\n",
    "        st = st.replace(c,\"'\")\n",
    "    for c in d_quotes:\n",
    "        st = st.replace(c,'\"')\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: replace_quotes(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: replace_quotes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We preprocess gargabe characters in the content of our snippets and claims\n",
    "\n",
    "garbage = \"‚Ä¶[]‚Ä¢‚Ñ¢üíïüôÇüáÆüá©‚Üí‚ô¶‚ò∫~‚àº^‚òÖ‚âà‚â•‚åÇ‚Ñ†‚Äû‚ô´‚äï‚Ä†‚òÜ¬Æ¬©¬¨„Äá...\"\n",
    "\n",
    "def rm_garbage(st):\n",
    "    global garbage\n",
    "    for c in garbage:\n",
    "        st = st.replace(c,'')\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: rm_garbage(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: rm_garbage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We lower the content of our snippets and claims\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.lower())\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We preprocess format errors on the content of our snippets and claims \n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢‚Ç¨‚Ñ¢s',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('‚Ç¨‚Ç¨‚Ç¨',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢‚Ç¨≈ì',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢‚Ç¨',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\n',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\r',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\t',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\',''))\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢‚Ç¨‚Ñ¢s',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('‚Ç¨‚Ç¨‚Ç¨',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢‚Ç¨≈ì',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢‚Ç¨',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\n',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\r',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\t',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We preprocess abbreviations on the content of our snippets and claims\n",
    "\n",
    "usa_abr = ['u.s.','u.s','u.s‚Ä¶','u.s.-','u.s.‚Äî','-u.s.', \n",
    " 'u.s..', '\\'u.s']\n",
    "unitednations_abr = ['u.n.']\n",
    "losangeles_abr = ['l.a.', 'l.a','l.a.','l.a']\n",
    "unitedkingdom_abr = ['u.k']\n",
    "newyork_abr = ['n.y.','n.y']\n",
    "europeanunion_abr = ['e.u']\n",
    "\n",
    "def replace_abr(st):\n",
    "    for abr in usa_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr,'united states')\n",
    "    for abr in unitednations_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr,'united nations')\n",
    "    for abr in losangeles_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr,'los angeles')\n",
    "    for abr in unitedkingdom_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr, 'united kingdom')\n",
    "    for abr in newyork_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr, 'new york')\n",
    "    for abr in europeanunion_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr, 'european union')\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: replace_abr(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: replace_abr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We preprocess separating characters (e.g. and/or -> and or)\n",
    "\n",
    "str_sep = '*:‚Ä¢/_‚Äì‚Äî\\\\-‚Äê,.|‚Äë‚Ä¶‚Üí]‚Äï=[~+'\n",
    "\n",
    "def rm_sep(st):\n",
    "    for c in str_sep:\n",
    "        if c in st:\n",
    "            st = ' '.join(st.split(c))\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: rm_sep(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: rm_sep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We tokenize the contents of our snippets and claims\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: nltk.word_tokenize(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We apply pos-tagging on our snippets and claims\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: nltk.pos_tag(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRP$', 'IN', ')', 'PDT', 'RBR', 'NNPS', '.', \"''\", 'VBD', 'MD', 'UH', '(', 'FW', 'RB', 'TO', '``', '#', 'JJ', 'VB', 'WDT', 'RBS', ':', 'VBG', 'PRP', 'WRB', 'DT', 'NNS', 'WP', 'EX', 'NNP', 'CC', 'RP', 'JJR', 'VBP', 'VBN', 'NN', 'POS', 'CD', 'WP$', '$', 'JJS', 'VBZ']\n"
     ]
    }
   ],
   "source": [
    "## Here is a list of all the pos-tags\n",
    "\n",
    "print(list(set([x[1] for lst in df_snippets['claim_content'] for x in lst])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove pos-tags that we consider irrelevant\n",
    "\n",
    "worthless_pos_tags = [',', 'PRP$', 'WRB', 'RP', 'TO','(','SYM','WDT', '``', 'WP$', ')', 'EX',\n",
    "                     'LS', ':', 'WP', 'MD', 'CD', '$', 'IN', '#', \"''\", 'FW', 'POS', 'DT',\n",
    "                     '.', 'PDT', 'CC', 'UH', 'PRP']\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x[1] not in worthless_pos_tags,tb)))\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda tb: [x[0] for x in tb])\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x[1] not in worthless_pos_tags,tb)))\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda tb: [x[0] for x in tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We keep only words that match a pattern of interest\n",
    "\n",
    "pattern_interest = re.compile(\"^[a-z']+$\")\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: pattern_interest.match(x), tb)))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: pattern_interest.match(x), tb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymerej/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "## We identify phrases (couple of words appearing frequently together)\n",
    "\n",
    "bigrams = Phrases(list(df_snippets['snippet_content'].as_matrix()))\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda tb: bigrams[tb])\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda tb: bigrams[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_claims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c3db429beebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_snippets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'snippet_content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_snippets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'snippet_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_snippets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_claims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_claims' is not defined"
     ]
    }
   ],
   "source": [
    "## We remove stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x not in stopWords, tb)))\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x not in stopWords, tb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We export our new data set\n",
    "\n",
    "df_snippets.to_csv('data/02_bow_snippets_claims.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194658\n"
     ]
    }
   ],
   "source": [
    "## We create an index of all the terms in the contents of our snippets and claims\n",
    "\n",
    "dictionary = corpora.Dictionary(list(df_snippets['snippet_content'].as_matrix()))\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "30000\n",
      "25000\n",
      "20000\n",
      "15000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "## We create different lexicon by selecting the more frequent terms\n",
    "\n",
    "for nb_word in sorted(list(np.arange(10000,40000,5000)),reverse=True):\n",
    "    dictionary.filter_extremes(keep_n=nb_word)\n",
    "    dictionary.save('dictionaries/02_'+str(nb_word/1000)+'.dict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
