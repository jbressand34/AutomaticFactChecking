{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "#https://www.datascience.com/resources/notebooks/word-embeddings-in-python\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>snippet_content</th>\n",
       "      <th>snippet_date</th>\n",
       "      <th>snippet_id</th>\n",
       "      <th>snippet_pagenum</th>\n",
       "      <th>snippet_title</th>\n",
       "      <th>snippet_url</th>\n",
       "      <th>claim_content</th>\n",
       "      <th>claim_date</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>claim_tag</th>\n",
       "      <th>claim_url</th>\n",
       "      <th>date_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>News on Japan, Business News, Opinion, Sports,...</td>\n",
       "      <td>17636</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Article expired | The Japan Times</td>\n",
       "      <td>https://www.japantimes.co.jp/article-expired/</td>\n",
       "      <td>Black and Latino people in NYC are arrested at...</td>\n",
       "      <td>17646</td>\n",
       "      <td>True</td>\n",
       "      <td>‚Äî PolitiFact New York</td>\n",
       "      <td>/new-york/statements/2018/apr/25/kirsten-gilli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Get the latest breaking news across the U.S. o...</td>\n",
       "      <td>17636</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>U.S. News | Latest National News, Videos ‚Ä¶</td>\n",
       "      <td>https://abcnews.go.com/US/</td>\n",
       "      <td>Black and Latino people in NYC are arrested at...</td>\n",
       "      <td>17646</td>\n",
       "      <td>True</td>\n",
       "      <td>‚Äî PolitiFact New York</td>\n",
       "      <td>/new-york/statements/2018/apr/25/kirsten-gilli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                                    snippet_content  snippet_date  \\\n",
       "0         3  News on Japan, Business News, Opinion, Sports,...         17636   \n",
       "1         3  Get the latest breaking news across the U.S. o...         17636   \n",
       "\n",
       "   snippet_id  snippet_pagenum                               snippet_title  \\\n",
       "0           0                0           Article expired | The Japan Times   \n",
       "1           1                0  U.S. News | Latest National News, Videos ‚Ä¶   \n",
       "\n",
       "                                     snippet_url  \\\n",
       "0  https://www.japantimes.co.jp/article-expired/   \n",
       "1                     https://abcnews.go.com/US/   \n",
       "\n",
       "                                       claim_content  claim_date  claim_label  \\\n",
       "0  Black and Latino people in NYC are arrested at...       17646         True   \n",
       "1  Black and Latino people in NYC are arrested at...       17646         True   \n",
       "\n",
       "                claim_tag                                          claim_url  \\\n",
       "0  ‚Äî PolitiFact New York   /new-york/statements/2018/apr/25/kirsten-gilli...   \n",
       "1  ‚Äî PolitiFact New York   /new-york/statements/2018/apr/25/kirsten-gilli...   \n",
       "\n",
       "   date_number  \n",
       "0            0  \n",
       "1            0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_snippets = pd.read_csv('datasets/00_snippets.csv')\n",
    "df_snippets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_quotes = ['`','¬¥',' π',' ª',' º',' Ω','Àä','Àã','À¥','Õ¥','ŒÑ','’ô','’ö','’õ','’ú','’ù','‚Äò','‚Äô','‚Äõ','‚Ä≤','‚Äµ', 'ﬂ¥','◊ô' ,'◊≥', 'ﬂµ']\n",
    "d_quotes = [' ∫','Àù','ÀÆ','Àµ','À∂','◊≤','◊¥', '‚Äú', '‚Äù', '‚Äü', '‚Ä≥', '‚Ä¥', '‚Ä∂', '‚Ä∑','``']\n",
    "\n",
    "def replace_quotes(st):\n",
    "    global s_quotes, d_quotes\n",
    "    for c in s_quotes:\n",
    "        st = st.replace(c,\"'\")\n",
    "    for c in d_quotes:\n",
    "        st = st.replace(c,'\"')\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: replace_quotes(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: replace_quotes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage = \"‚Ä¶[]‚Ä¢‚Ñ¢üíïüôÇüáÆüá©‚Üí‚ô¶‚ò∫~‚àº^‚òÖ‚âà‚â•‚åÇ‚Ñ†‚Äû‚ô´‚äï‚Ä†‚òÜ¬Æ¬©¬¨„Äá...\"\n",
    "\n",
    "def rm_garbage(st):\n",
    "    global garbage\n",
    "    for c in garbage:\n",
    "        st = st.replace(c,'')\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: rm_garbage(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: rm_garbage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.lower())\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢‚Ç¨‚Ñ¢s',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('‚Ç¨‚Ç¨‚Ç¨',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢‚Ç¨≈ì',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢‚Ç¨',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('√¢',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\n',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\r',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\t',''))\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: x.replace('\\\\',''))\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢‚Ç¨‚Ñ¢s',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('‚Ç¨‚Ç¨‚Ç¨',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢‚Ç¨≈ì',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢‚Ç¨',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('√¢',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\n',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\r',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\t',''))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: x.replace('\\\\',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_abr = ['u.s.','u.s','u.s‚Ä¶','u.s.-','u.s.‚Äî','-u.s.', \n",
    " 'u.s..', '\\'u.s']\n",
    "unitednations_abr = ['u.n.']\n",
    "losangeles_abr = ['l.a.', 'l.a','l.a.','l.a']\n",
    "unitedkingdom_abr = ['u.k']\n",
    "newyork_abr = ['n.y.','n.y']\n",
    "europeanunion_abr = ['e.u']\n",
    "\n",
    "def replace_abr(st):\n",
    "    for abr in usa_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr,'united states')\n",
    "    for abr in unitednations_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr,'united nations')\n",
    "    for abr in losangeles_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr,'los angeles')\n",
    "    for abr in unitedkingdom_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr, 'united kingdom')\n",
    "    for abr in newyork_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr, 'new york')\n",
    "    for abr in europeanunion_abr:\n",
    "        if abr in st:\n",
    "            st = st.replace(abr, 'european union')\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: replace_abr(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: replace_abr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_sep = '*:‚Ä¢/_‚Äì‚Äî\\\\-‚Äê,.|‚Äë‚Ä¶‚Üí]‚Äï=[~+'\n",
    "\n",
    "def rm_sep(st):\n",
    "    for c in str_sep:\n",
    "        if c in st:\n",
    "            st = ' '.join(st.split(c))\n",
    "    return st\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: rm_sep(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: rm_sep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: nltk.word_tokenize(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: nltk.pos_tag(x))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VBN', '.', 'VBZ', 'NNP', 'IN', 'VB', 'VBG', 'WDT', 'FW', 'RP', 'TO', '(', 'UH', \"''\", 'NNPS', 'NNS', 'NN', 'RB', 'DT', 'CC', 'WRB', 'EX', 'WP', '#', 'RBS', 'JJS', ':', 'PRP', ')', 'CD', 'JJR', 'MD', 'JJ', 'VBP', 'PDT', 'WP$', '``', 'POS', '$', 'VBD', 'RBR', 'PRP$']\n"
     ]
    }
   ],
   "source": [
    "print(list(set([x[1] for lst in df_snippets['claim_content'] for x in lst])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "worthless_pos_tags = [',', 'PRP$', 'WRB', 'RP', 'TO','(','SYM','WDT', '``', 'WP$', ')', 'EX',\n",
    "                     'LS', ':', 'WP', 'MD', 'CD', '$', 'IN', '#', \"''\", 'FW', 'POS', 'DT',\n",
    "                     '.', 'PDT', 'CC', 'UH', 'PRP']\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x[1] not in worthless_pos_tags,tb)))\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda tb: [x[0] for x in tb])\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x[1] not in worthless_pos_tags,tb)))\n",
    "\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda tb: [x[0] for x in tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_interest = re.compile(\"^[a-z']+$\")\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: pattern_interest.match(x), tb)))\n",
    "df_snippets['claim_content'] = df_snippets['claim_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: pattern_interest.match(x), tb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymerej/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigrams = Phrases(list(df_snippets['snippet_content'].as_matrix()))\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda tb: bigrams[tb])\n",
    "df_snippets['claim_content'] = df_snippets['claim_content'].apply(lambda tb: bigrams[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "df_snippets['snippet_content'] = df_snippets['snippet_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x not in stopWords, tb)))\n",
    "\n",
    "df_snippets['claim_content'] = df_claims['claim_content']\\\n",
    ".apply(lambda tb: list(filter(lambda x: x not in stopWords, tb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snippets.to_csv('datasets/02_bow_snippets_claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymerej/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_snippets = pd.read_csv('datasets/02_bow_snippets_claims.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snippets['snippet_content'] = df_snippets['snippet_content'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194658\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(list(df_snippets['snippet_content'].as_matrix()))\n",
    "print(len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113764\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n = 194658)\n",
    "print(len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "30000\n",
      "25000\n",
      "20000\n",
      "15000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for nb_word in sorted(list(np.arange(10000,40000,5000)),reverse=True):\n",
    "    dictionary.filter_extremes(keep_n=nb_word)\n",
    "    dictionary.save('dictionaries/02_'+str(nb_word/1000)+'.dict')\n",
    "    print(len(dictionary.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclaims = df_snippets[['claim_id','claim_content','claim_label']].groupby(['claim_id'])\\\n",
    ".first().reset_index()[['claim_content','claim_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 7, 10, 12, 7, 11, 9, 7, 7, 13]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_word = 35000\n",
    "dictionary = corpora.Dictionary.load('dictionaries/02_'+str(nb_word/1000)+'.dict')\n",
    "[len(dictionary.doc2bow(dfclaims['claim_content'][i])) for i in dfclaims.index[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, nan]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: type(x) != type([]),list(dfclaims['claim_content'].as_matrix())))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2eb0384cbe04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dictionaries/02_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_word\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdfclaims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfclaims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-2eb0384cbe04>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dictionaries/02_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_word\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdfclaims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfclaims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim_content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "for nb_word in sorted(list(np.arange(10000,40000,5000)),reverse=True):\n",
    "    dictionary = corpora.Dictionary.load('dictionaries/02_'+str(nb_word/1000)+'.dict')\n",
    "    dfclaims[str(nb_word)] = dfclaims['claim_content'].apply(lambda x: len(dictionary.doc2bow(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
